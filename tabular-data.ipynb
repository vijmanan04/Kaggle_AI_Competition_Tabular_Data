{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/tabular-playground-series-jan-2021/train.csv\"\ntest_path = \"/kaggle/input/tabular-playground-series-jan-2021/test.csv\"\ndf_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ncol = df_train.columns\ndf_test.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build simple linear regression and see accuracy\nfrom sklearn import linear_model\nX = df_train[col[1:15]]\ny = df_train['target']\n\nlinear = linear_model.LinearRegression(fit_intercept = True)\n\nlinear.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#linear model feature importance\nfrom matplotlib import pyplot\ncoef = linear.coef_\n\nstdev = []\nfor i in range(len(df_train.columns)):\n    if i != 0 and i != 15:\n        stdev.append(df_train[col[i]].std())\n        \ndf_importance = []\nfor i,v in enumerate(coef):\n    importance = coef[i] * stdev[i]\n    df_importance.append(importance)\n    print('Feature: %0d, Score: %.5f' % (i,importance))\ndf_importance = pd.DataFrame(df_importance)\ndf_importance\n    \npyplot.bar([x for x in range(len(df_importance))], abs(df_importance[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_normalized = (X-X.mean())/X.std()\n\nlinear.fit(X_normalized, y)\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = linear.predict(df_test[col[1:15]])\npred = pd.DataFrame(y_pred)\nsubmission = pd.concat([df_test['id'], pred], axis = 1)\nsubmission.columns = ['id', 'target']\nsubmission.to_csv(\"normalized.csv\", index = False)\n#RMSE Error: 0.73333","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred = linear.predict(df_test[col[1:15]])\npred = pd.DataFrame(y_pred)\nsubmission = pd.concat([df_test['id'], pred], axis = 1)\nsubmission.columns = ['id', 'target']\nsubmission.to_csv(\"first_submission.csv\", index = False)\n#RMSE Error: 0.72782","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_train, X_train_test = train_test_split(df_train, test_size=0.20, random_state=1)\nX_train_test_values = pd.DataFrame(X_train_test[col[1:15]])\nX_train_test_target = pd.DataFrame(X_train_test['target'])\n\nX_train_train_values = pd.DataFrame(X_train_train[col[1:15]])\nX_train_train_target = pd.DataFrame(X_train_train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optimize linear regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(5, 3, figsize = (20,25))\n#x = df_train[col[4]]\ny = df_train[\"target\"]\n\nindex = 1\nfor i in range(5):\n    for j in range(3):\n        x = df_train[col[index]]\n        sns.scatterplot(ax = axes[i, j], x = x[0:20000], y = y[0:20000])\n        axes[i][j].set_title(f\"Plot for cont{index}\")\n        index += 1\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig2, axes2 = plt.subplots(5, 3, figsize = (20,25))\n\nindex = 1\nfor i in range(5):\n    for j in range(3):\n        sns.distplot(df_train[col[index]], ax = axes2[i][j])\n        axes[i][j].set_title(f\"Plot for cont{index}\")\n        index += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_transformed = []\nfor i in range(len(df_train.columns)):\n    log_transformed.append(np.log1p(df_train[col[i]]))\nlog_transformed = pd.DataFrame(log_transformed).T\n\nfig2, axes2 = plt.subplots(5, 3, figsize = (20,25))\n\nindex = 1\nfor i in range(5):\n    for j in range(3):\n        sns.distplot(log_transformed[col[index]], ax = axes2[i][j])\n        axes[i][j].set_title(f\"Plot for cont{index}\")\n        index += 1\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nlinear_pred = linear.predict(X_train_test_values)\nscore_linear = mean_squared_error(X_train_test_target, linear_pred,squared=False)\nprint(f'{score_linear:0.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tree models\nfrom sklearn.tree import DecisionTreeRegressor\nd_tree = DecisionTreeRegressor(random_state=0)\nd_tree.fit(X_train_train_values, X_train_train_target)\ny_dtree = d_tree.predict(X_train_test_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_dtree = mean_squared_error(X_train_test_target, y_dtree, squared=False)\nprint(f'{score_dtree:0.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pd.DataFrame(y_dtree)\nplt.figure(figsize = (10,5))\nsns.scatterplot(X_train_test_target['target'], y_pred[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random forest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nestimators = [30, 50, 60, 70]\nmax_depth = [None, 2, 3]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\naccuracy = []\nfor i in estimators:\n    for j in max_depth:\n        for a in min_samples_split:\n            for b in min_samples_leaf:\n                for c in bootstrap:\n                    rf_model = RandomForestRegressor(n_estimators=i, n_jobs=-1, max_depth = j, min_samples_split = a, min_samples_leaf = b, bootstrap = c)\n                    rf_model.fit(X_train_train_values, X_train_train_target)\n\n                    y_rf = rf_model.predict(X_train_test_values)\n                    score_rf = mean_squared_error(X_train_test_target, y_rf, squared=False)\n                    accuracy.append([i, j, a, b, c, score_rf])\n                    print(i, j, a, b, c, score_rf)\nresults = pd.DataFrame(data = accuracy, columns = [\"estimator\", \"max_depth\", \"samples_split\", \"samples_leaf\", \"bootstrap\", \"score\"])\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 0, stop = 100, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 2, verbose=10, random_state=42, n_jobs = -1)\nrf_random.fit(X_train_train_values, X_train_train_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = tf.keras.Sequential( … ) # define your model normally\n    model.compile( … )\n\n# train model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.sort_values(by = ['score'], ascending = True)\n#best combo = 70, NaN for estimator and max_depth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train[col[1:15]]\ny = df_train['target']\n\nrf_model = RandomForestRegressor(n_estimators=70, n_jobs=-1, max_depth = None)\n\nrf_model.fit(X, y)\n\nrf_y_pred = rf_model.predict(df_test[col[1:15]])\n\npred = pd.DataFrame(rf_y_pred)\nsubmission = pd.concat([df_test['id'], pred], axis = 1)\nsubmission.columns = ['id', 'target']\nsubmission.to_csv(\"rf_model.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rf = pd.DataFrame(y_rf)\nplt.figure(figsize = (10,5))\nsns.scatterplot(X_train_test_target['target'], y_pred_rf[0])\nplt.plot([4, 10], [4,10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_rf = rf_model.feature_importances_ \n\nimportance_df = []\n# summarize feature importance\nfor i,v in enumerate(importance_rf):\n    print('Feature: %0d, Score: %.5f' % (i+1,v))\n    importance_df.append(v)\n# plot feature importance\nplt.ylim([0.0675, 0.08])\npyplot.bar([x for x in range(len(importance_rf))], abs(importance_rf))\npyplot.show()\nimportance_df = pd.DataFrame( data = importance_df).sort_values(by = [0], ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Important features: 0- 3, 6, 9, 12\nimportant_features_rf = pd.DataFrame([df_train[col[2]], df_train[col[3]], df_train[col[4]], df_train[col[7]], df_train[col[10]], df_train[col[13]]]).T\nimportant_features_rf.columns = [\"cont2\", \"cont3\", \"cont4\", \"cont7\", \"cont10\", \"cont13\" ]\nimportant_features_rf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_rf = pd.DataFrame(importance_rf)\nsns.set_palette(\"husl\")\nsns.heatmap(df_train[col[1:16]].corr(), cmap = \"PiYG\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.bar([x for x in range(len(df_importance))], abs(df_importance[0]))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature creation\nimportant_features_rf_squared = important_features_rf ** 2\nimportant_features_rf_squared.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\" ]\nmult_3_4 = important_features_rf[\"cont3\"] * important_features_rf[\"cont4\"]\nmult_3_4.columns = [\"3*4\"]\nmult_2_3 = important_features_rf[\"cont3\"] * important_features_rf[\"cont2\"]\nmult_2_3.columns = [\"2*3\"]\nmult_2_4 = important_features_rf[\"cont2\"] * important_features_rf[\"cont4\"]\nmult_2_4.columns = [\"2*4\"]\nnew_important_features = pd.concat([important_features_rf_squared, mult_3_4, mult_2_3, mult_2_4], axis = 1)\nnew_important_features.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\", \"3*4\", \"2*3\", \"2*4\"]\nnew_important_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(new_important_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_train = pd.concat([df_train, new_important_features], axis = 1)\nrf_train_values = rf_train.drop(['id', 'target'], axis = 1)\ncol_train = rf_train_values.columns\n#normal distribution\nlog_transformed = []\nfor i in range(len(rf_train_values.columns)):\n    log_transformed.append(np.log1p(rf_train_values[col_train[i]]))\nlog_transformed_rf_train_values = pd.DataFrame(log_transformed).T\n\n\n\nrf_train_target = rf_train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators=70, n_jobs=-1, max_depth = None)\n\nrf_model.fit(log_transformed_rf_train_values, rf_train_target)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare validation data\nvalidation_xgb = pd.DataFrame(X_train_test[col[0:15]])\nX_train_test_target = pd.DataFrame(X_train_test['target'])\n\n#get test data prepared\nvalidation = pd.DataFrame([validation_xgb[col[2]], validation_xgb[col[3]], validation_xgb[col[4]], validation_xgb[col[7]], validation_xgb[col[10]], validation_xgb[col[12]]]).T\nvalidation.columns = [\"cont2\", \"cont3\", \"cont4\", \"cont7\", \"cont10\", \"cont13\" ]\n\nimportant_validation_squared = validation ** 2\nimportant_validation_squared.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\" ]\nmult_3_4_validation = validation[\"cont3\"] * validation[\"cont4\"]\nmult_3_4_validation.columns = [\"3*4\"]\nmult_2_3_validation = validation[\"cont3\"] * validation[\"cont2\"]\nmult_2_3_validation.columns = [\"2*3\"]\nmult_2_4_validation = validation[\"cont2\"] * validation[\"cont4\"]\nmult_2_4_validation.columns = [\"2*4\"]\nnew_important_features_validation = pd.concat([important_validation_squared, mult_3_4_validation, mult_2_3_validation, mult_2_4_validation], axis = 1)\nnew_important_features_validation.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\", \"3*4\", \"2*3\", \"2*4\"]\nnew_important_features_validation\n\n\nxgb_val = pd.concat([validation_xgb, new_important_features_validation], axis = 1)\nxgb_val = xgb_val.drop(['id'], axis = 1)\ncol_val = xgb_val.columns\n\n#normal distribution\nlog_transformed = []\nfor i in range(len(xgb_val.columns)):\n    log_transformed.append(np.log1p(xgb_val[col_test[i]]))\nvalidation_dataset = pd.DataFrame(log_transformed).T\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get test data prepared\ntest_important = pd.DataFrame([df_test[col[2]], df_test[col[3]], df_test[col[4]], df_test[col[7]], df_test[col[10]], df_test[col[12]]]).T\ntest_important.columns = [\"cont2\", \"cont3\", \"cont4\", \"cont7\", \"cont10\", \"cont13\" ]\n\nimportant_features_test_squared = test_important ** 2\nimportant_features_test_squared.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\" ]\nmult_3_4_test = test_important[\"cont3\"] * test_important[\"cont4\"]\nmult_3_4_test.columns = [\"3*4\"]\nmult_2_3_test = test_important[\"cont3\"] * test_important[\"cont2\"]\nmult_2_3_test.columns = [\"2*3\"]\nmult_2_4_test = test_important[\"cont2\"] * test_important[\"cont4\"]\nmult_2_4_test.columns = [\"2*4\"]\nnew_important_features_test = pd.concat([important_features_test_squared, mult_3_4_test, mult_2_3_test, mult_2_4_test], axis = 1)\nnew_important_features_test.columns = [\"cont2_sq\", \"cont3_sq\", \"cont4_sq\", \"cont7_sq\", \"cont10_sq\", \"cont13_sq\", \"3*4\", \"2*3\", \"2*4\"]\nnew_important_features_test\n\n\nrf_test = pd.concat([df_test, new_important_features_test], axis = 1)\nrf_test = rf_test.drop(['id'], axis = 1)\ncol_test = rf_test.columns\n\n#normal distribution\nlog_transformed = []\nfor i in range(len(rf_test.columns)):\n    log_transformed.append(np.log1p(rf_test[col_test[i]]))\nprediction_dataset = pd.DataFrame(log_transformed).T\nprediction_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = rf_model.predict(prediction_dataset)\n\npred = pd.DataFrame(pred)\nsubmission = pd.concat([df_test['id'], pred], axis = 1)\nsubmission.columns = ['id', 'target']\nsubmission.to_csv(\"rf_model2.csv\", index = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nmodel_XGB = XGBRegressor(                 \n                colsample_bytree=0.5,\n                 alpha=0.01563,\n                 #gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=15,\n                 min_child_weight=257,\n                 n_estimators=1000,                                                                  \n                 #reg_alpha=0.9,\n                 reg_lambda=0.003,\n                 subsample=0.7,\n                 random_state=2020,\n                 metric_period=100,\n                 silent=1,\n                n_jobs = -1)\nprint('ready to train')\nmodel_XGB.fit(log_transformed_rf_train_values, rf_train_target, early_stopping_rounds=6, eval_set=[(validation_dataset, X_train_test_target)], verbose=1)\nprint(\"predicting\")\ny_xgb = model_XGB.predict(validation_dataset)\n\nscore_xgb = mean_squared_error(X_train_test_target, y_xgb, squared=False)\nprint(f'{score_xgb:0.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"predicting\")\ny_xgb = model_XGB.predict(validation_dataset)\n\nscore_xgb = mean_squared_error(X_train_test_target, y_xgb, squared=False)\nprint(f'{score_xgb:0.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_XGB.fit(log_transformed_rf_train_values, rf_train_target)\nxgb_pred = model_XGB.predict(prediction_dataset)\n\npred = pd.DataFrame(xgb_pred)\nsubmission = pd.concat([df_test['id'], pred], axis = 1)\nsubmission.columns = ['id', 'target']\nsubmission.to_csv(\"xgb2.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}